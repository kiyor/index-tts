#!/usr/bin/env python3
"""
IndexTTS WebUI - ç»Ÿä¸€ç‰ˆæœ¬
åŒ…å« bitsandbytes å…¼å®¹æ€§ä¿®å¤ã€Docker æ”¯æŒã€Demos éŸ³é¢‘é€‰æ‹©åŠŸèƒ½ã€ç³»ç»Ÿä¿¡æ¯å’Œé˜Ÿåˆ—ç®¡ç†
"""

# ä¿®å¤ bitsandbytes å…¼å®¹æ€§é—®é¢˜
import sys
sys.modules['bitsandbytes'] = None

import json
import os
import sys
import threading
import time
import glob
import queue
import uuid
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
from collections import deque
import statistics

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, "indextts"))

import argparse
parser = argparse.ArgumentParser(description="IndexTTS WebUI (ç»Ÿä¸€ç‰ˆæœ¬)")
parser.add_argument("--verbose", action="store_true", default=False, help="Enable verbose mode")
parser.add_argument("--port", type=int, default=7860, help="Port to run the web UI on")
parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to run the web UI on (0.0.0.0 for Docker)")
parser.add_argument("--model_dir", type=str, default="checkpoints", help="Model checkpoints directory")
parser.add_argument("--share", action="store_true", default=False, help="Create a publicly shareable link")
cmd_args = parser.parse_args()

print("ğŸš€ IndexTTS WebUI (ç»Ÿä¸€ç‰ˆæœ¬)")
print("=" * 50)

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
if not os.path.exists(cmd_args.model_dir):
    print(f"âŒ æ¨¡å‹ç›®å½• {cmd_args.model_dir} ä¸å­˜åœ¨ã€‚è¯·å…ˆä¸‹è½½æ¨¡å‹ã€‚")
    sys.exit(1)

required_files = [
    "bigvgan_generator.pth",
    "bpe.model", 
    "gpt.pth",
    "config.yaml",
]

missing_files = []
for file in required_files:
    file_path = os.path.join(cmd_args.model_dir, file)
    if not os.path.exists(file_path):
        missing_files.append(file_path)

if missing_files:
    print("âŒ ç¼ºå°‘å¿…éœ€æ–‡ä»¶:")
    for file in missing_files:
        print(f"   - {file}")
    print("\nè¯·ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ–‡ä»¶éƒ½å·²ä¸‹è½½åˆ° checkpoints ç›®å½•")
    sys.exit(1)

print("âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")

# æ£€æŸ¥ GPU
try:
    import torch
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ª GPU: {gpu_name}")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼")
except Exception as e:
    print(f"âš ï¸  GPU æ£€æŸ¥å¤±è´¥: {e}")

import gradio as gr

try:
    from indextts.infer import IndexTTS
    print("âœ… IndexTTS å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ IndexTTS å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# å°è¯•å¯¼å…¥ i18nï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ
try:
    from tools.i18n.i18n import I18nAuto
    i18n = I18nAuto(language="zh_CN")
    print("âœ… i18n æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError:
    print("âš ï¸  i18n æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€")
    class SimpleI18n:
        def __call__(self, text):
            return text
    i18n = SimpleI18n()

# ä»»åŠ¡çŠ¶æ€æšä¸¾
@dataclass
class TaskStatus:
    QUEUED = "queued"      # æ’é˜Ÿä¸­
    RUNNING = "running"    # æ‰§è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"      # å¤±è´¥

@dataclass
class Task:
    """ä»»åŠ¡æ•°æ®ç»“æ„"""
    id: str
    prompt: str
    text: str
    infer_mode: str
    params: Dict[str, Any]
    status: str = TaskStatus.QUEUED
    created_time: float = 0
    start_time: float = 0
    end_time: float = 0
    result_path: Optional[str] = None
    error_message: Optional[str] = None
    
    def __post_init__(self):
        if self.created_time == 0:
            self.created_time = time.time()

class TaskQueue:
    """ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨"""
    
    def __init__(self, max_history=50):
        self.queue = queue.Queue()
        self.current_task: Optional[Task] = None
        self.task_history: deque = deque(maxlen=max_history)
        self.execution_times: deque = deque(maxlen=20)  # ä¿å­˜æœ€è¿‘20æ¬¡æ‰§è¡Œæ—¶é—´ç”¨äºé¢„ä¼°
        self.worker_thread = None
        self.is_running = False
        self.lock = threading.Lock()
        
    def start_worker(self, tts_instance):
        """å¯åŠ¨é˜Ÿåˆ—å¤„ç†å·¥ä½œçº¿ç¨‹"""
        if self.worker_thread is None or not self.worker_thread.is_alive():
            self.is_running = True
            self.worker_thread = threading.Thread(target=self._worker, args=(tts_instance,), daemon=True)
            self.worker_thread.start()
            
    def stop_worker(self):
        """åœæ­¢é˜Ÿåˆ—å¤„ç†å·¥ä½œçº¿ç¨‹"""
        self.is_running = False
        
    def add_task(self, prompt: str, text: str, infer_mode: str, params: Dict[str, Any]) -> str:
        """æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        task = Task(
            id=str(uuid.uuid4())[:8],
            prompt=prompt,
            text=text,
            infer_mode=infer_mode,
            params=params
        )
        
        with self.lock:
            self.queue.put(task)
            
        print(f"ğŸ“ ä»»åŠ¡ {task.id} å·²åŠ å…¥é˜Ÿåˆ—")
        return task.id
        
    def get_queue_status(self) -> Dict[str, Any]:
        """è·å–é˜Ÿåˆ—çŠ¶æ€ä¿¡æ¯"""
        with self.lock:
            queue_size = self.queue.qsize()
            current_task_info = None
            
            if self.current_task:
                elapsed = time.time() - self.current_task.start_time
                estimated_remaining = self._estimate_remaining_time()
                current_task_info = {
                    'id': self.current_task.id,
                    'text_preview': self.current_task.text[:50] + ('...' if len(self.current_task.text) > 50 else ''),
                    'elapsed_time': elapsed,
                    'estimated_remaining': estimated_remaining
                }
                
            return {
                'queue_size': queue_size,
                'current_task': current_task_info,
                'total_completed': len([t for t in self.task_history if t.status == TaskStatus.COMPLETED]),
                'average_execution_time': statistics.mean(self.execution_times) if self.execution_times else 0,
                'estimated_wait_time': self._estimate_wait_time(queue_size)
            }
            
    def get_task_result(self, task_id: str) -> Optional[Task]:
        """æ ¹æ®ä»»åŠ¡IDè·å–ä»»åŠ¡ç»“æœ"""
        with self.lock:
            # æ£€æŸ¥å½“å‰ä»»åŠ¡
            if self.current_task and self.current_task.id == task_id:
                return self.current_task
                
            # æ£€æŸ¥å†å²ä»»åŠ¡
            for task in self.task_history:
                if task.id == task_id:
                    return task
                    
        return None
        
    def _worker(self, tts_instance):
        """é˜Ÿåˆ—å¤„ç†å·¥ä½œçº¿ç¨‹"""
        print("ğŸ”„ é˜Ÿåˆ—å¤„ç†å™¨å·²å¯åŠ¨")
        
        while self.is_running:
            try:
                # ç­‰å¾…ä»»åŠ¡ï¼Œè¶…æ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                task = self.queue.get(timeout=1)
                
                with self.lock:
                    self.current_task = task
                    task.status = TaskStatus.RUNNING
                    task.start_time = time.time()
                
                print(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task.id}")
                
                try:
                    # æ‰§è¡Œä»»åŠ¡
                    output_path = os.path.join("outputs", f"task_{task.id}_{int(time.time())}.wav")
                    
                    # æ ¹æ®æ¨ç†æ¨¡å¼è¿‡æ»¤å‚æ•°
                    if task.infer_mode == "æ™®é€šæ¨ç†":
                        # æ™®é€šæ¨ç†ä¸æ”¯æŒ sentences_bucket_max_size å‚æ•°
                        infer_params = {k: v for k, v in task.params.items() 
                                      if k != 'sentences_bucket_max_size'}
                        result = tts_instance.infer(
                            task.prompt, 
                            task.text, 
                            output_path,
                            verbose=cmd_args.verbose,
                            **infer_params
                        )
                    else:
                        # æ‰¹æ¬¡æ¨ç†æ”¯æŒæ‰€æœ‰å‚æ•°
                        result = tts_instance.infer_fast(
                            task.prompt, 
                            task.text, 
                            output_path,
                            verbose=cmd_args.verbose,
                            **task.params
                        )
                    
                    # ä»»åŠ¡å®Œæˆ
                    with self.lock:
                        task.status = TaskStatus.COMPLETED
                        task.end_time = time.time()
                        task.result_path = result
                        
                        # è®°å½•æ‰§è¡Œæ—¶é—´ç”¨äºé¢„ä¼°
                        execution_time = task.end_time - task.start_time
                        self.execution_times.append(execution_time)
                        
                        # ç§»åŠ¨åˆ°å†å²è®°å½•
                        self.task_history.append(task)
                        self.current_task = None
                        
                    print(f"âœ… ä»»åŠ¡ {task.id} å®Œæˆï¼Œè€—æ—¶ {execution_time:.2f} ç§’")
                    
                except Exception as e:
                    # ä»»åŠ¡å¤±è´¥
                    with self.lock:
                        task.status = TaskStatus.FAILED
                        task.end_time = time.time()
                        task.error_message = str(e)
                        
                        # ç§»åŠ¨åˆ°å†å²è®°å½•
                        self.task_history.append(task)
                        self.current_task = None
                        
                    print(f"âŒ ä»»åŠ¡ {task.id} å¤±è´¥: {e}")
                    
                finally:
                    self.queue.task_done()
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ é˜Ÿåˆ—å¤„ç†å™¨é”™è¯¯: {e}")
                
        print("ğŸ›‘ é˜Ÿåˆ—å¤„ç†å™¨å·²åœæ­¢")
        
    def _estimate_remaining_time(self) -> float:
        """é¢„ä¼°å½“å‰ä»»åŠ¡å‰©ä½™æ—¶é—´"""
        if not self.current_task or not self.execution_times:
            return 0
            
        avg_time = statistics.mean(self.execution_times)
        elapsed = time.time() - self.current_task.start_time
        return max(0, avg_time - elapsed)
        
    def _estimate_wait_time(self, queue_size: int) -> float:
        """é¢„ä¼°æ’é˜Ÿç­‰å¾…æ—¶é—´"""
        if queue_size == 0 or not self.execution_times:
            return 0
            
        avg_time = statistics.mean(self.execution_times)
        current_remaining = self._estimate_remaining_time()
        return current_remaining + (queue_size * avg_time)

# å…¨å±€é˜Ÿåˆ—å®ä¾‹
task_queue = TaskQueue()

print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– IndexTTS...")
try:
    tts = IndexTTS(
        model_dir=cmd_args.model_dir, 
        cfg_path=os.path.join(cmd_args.model_dir, "config.yaml")
    )
    print("âœ… IndexTTS åˆå§‹åŒ–æˆåŠŸ")
    
    # å¯åŠ¨é˜Ÿåˆ—å¤„ç†å™¨
    task_queue.start_worker(tts)
    
except Exception as e:
    print(f"âŒ IndexTTS åˆå§‹åŒ–å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("outputs", exist_ok=True)
os.makedirs("outputs/tasks", exist_ok=True)
os.makedirs("prompts", exist_ok=True)
os.makedirs("logs", exist_ok=True)

# æ‰«ædemosç›®å½•è·å–éŸ³é¢‘æ–‡ä»¶
def scan_demos_directory():
    """æ‰«ædemosç›®å½•ï¼Œè¿”å›åˆ†ç±»å’ŒéŸ³é¢‘æ–‡ä»¶çš„å­—å…¸"""
    demos_dir = "demos"
    if not os.path.exists(demos_dir):
        if cmd_args.verbose:
            print(f"âš ï¸  demosç›®å½•ä¸å­˜åœ¨: {demos_dir}")
        return {}
    
    demos_dict = {}
    
    try:
        # éå†ä¸€çº§ç›®å½•ï¼ˆåˆ†ç±»ï¼‰
        for category in os.listdir(demos_dir):
            category_path = os.path.join(demos_dir, category)
            if not os.path.isdir(category_path) or category.startswith('.'):
                continue
                
            if cmd_args.verbose:
                print(f"ğŸ“ æ‰«æåˆ†ç±»: {category}")
            demos_dict[category] = {}
            
            # éå†äºŒçº§ç›®å½•ï¼ˆå­åˆ†ç±»ï¼‰
            for subcategory in os.listdir(category_path):
                subcategory_path = os.path.join(category_path, subcategory)
                if not os.path.isdir(subcategory_path) or subcategory.startswith('.'):
                    continue
                    
                if cmd_args.verbose:
                    print(f"  ğŸ“‚ æ‰«æå­åˆ†ç±»: {category}/{subcategory}")
                
                # æŸ¥æ‰¾wavæ–‡ä»¶
                wav_files = glob.glob(os.path.join(subcategory_path, "*.wav"))
                if wav_files:
                    demos_dict[category][subcategory] = []
                    for wav_file in sorted(wav_files):
                        filename = os.path.basename(wav_file)
                        if cmd_args.verbose:
                            print(f"    ğŸµ å‘ç°éŸ³é¢‘: {filename}")
                        demos_dict[category][subcategory].append({
                            'name': filename,
                            'path': wav_file
                        })
                else:
                    if cmd_args.verbose:
                        print(f"    âš ï¸  {category}/{subcategory} ç›®å½•ä¸­æœªæ‰¾åˆ°WAVæ–‡ä»¶")
    
    except Exception as e:
        print(f"âŒ æ‰«ædemosç›®å½•å¤±è´¥: {e}")
        if cmd_args.verbose:
            import traceback
            traceback.print_exc()
    
    return demos_dict

def get_demo_categories():
    """è·å–demosåˆ†ç±»åˆ—è¡¨ï¼ˆåŠ¨æ€æ‰«æï¼‰"""
    demos_dict = scan_demos_directory()
    return list(demos_dict.keys())

def get_demo_subcategories(category):
    """æ ¹æ®åˆ†ç±»è·å–å­åˆ†ç±»åˆ—è¡¨ï¼ˆåŠ¨æ€æ‰«æï¼‰"""
    demos_dict = scan_demos_directory()
    if category in demos_dict:
        return list(demos_dict[category].keys())
    return []

def get_demo_audio_files(category, subcategory):
    """æ ¹æ®åˆ†ç±»å’Œå­åˆ†ç±»è·å–éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨ï¼ˆåŠ¨æ€æ‰«æï¼‰"""
    demos_dict = scan_demos_directory()
    if category in demos_dict and subcategory in demos_dict[category]:
        return [audio['name'] for audio in demos_dict[category][subcategory]]
    return []

def get_demo_audio_path(category, subcategory, filename):
    """è·å–æŒ‡å®šéŸ³é¢‘æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼ˆåŠ¨æ€æ‰«æï¼‰"""
    demos_dict = scan_demos_directory()
    if category in demos_dict and subcategory in demos_dict[category]:
        for audio in demos_dict[category][subcategory]:
            if audio['name'] == filename:
                return audio['path']
    return None

def get_demos_statistics():
    """è·å–demosç»Ÿè®¡ä¿¡æ¯ï¼ˆåŠ¨æ€æ‰«æï¼‰"""
    demos_dict = scan_demos_directory()
    total_categories = len(demos_dict)
    total_files = sum(len(files) for subcats in demos_dict.values() for files in subcats.values())
    return total_categories, total_files, demos_dict

# åŠ è½½ç¤ºä¾‹æ¡ˆä¾‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
example_cases = []
if os.path.exists("tests/cases.jsonl"):
    try:
        with open("tests/cases.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                example = json.loads(line)
                # ç§»é™¤å‚è€ƒéŸ³é¢‘ï¼Œåªä¿ç•™æ–‡æœ¬å’Œæ¨ç†æ¨¡å¼
                example_cases.append([
                    example.get("text"), 
                    ["æ™®é€šæ¨ç†", "æ‰¹æ¬¡æ¨ç†"][example.get("infer_mode", 0)]
                ])
        print(f"âœ… åŠ è½½äº† {len(example_cases)} ä¸ªç¤ºä¾‹æ¡ˆä¾‹")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½ç¤ºä¾‹æ¡ˆä¾‹å¤±è´¥: {e}")

def gen_single_with_queue(prompt, text, infer_mode, max_text_tokens_per_sentence=120, sentences_bucket_max_size=4,
                         *args, progress=gr.Progress()):
    """ä½¿ç”¨é˜Ÿåˆ—çš„è¯­éŸ³ç”Ÿæˆå‡½æ•°"""
    if not prompt:
        gr.Warning("è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘æ–‡ä»¶æˆ–é€‰æ‹©é¢„è®¾éŸ³é¢‘")
        return gr.update(value=None, visible=True), "è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘"
    
    if not text or not text.strip():
        gr.Warning("è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬")
        return gr.update(value=None, visible=True), "è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬"
    
    # è§£æå‚æ•°
    do_sample, top_p, top_k, temperature, \
        length_penalty, num_beams, repetition_penalty, max_mel_tokens = args
    
    params = {
        "do_sample": bool(do_sample),
        "top_p": float(top_p),
        "top_k": int(top_k) if int(top_k) > 0 else None,
        "temperature": float(temperature),
        "length_penalty": float(length_penalty),
        "num_beams": int(num_beams),
        "repetition_penalty": float(repetition_penalty),
        "max_mel_tokens": int(max_mel_tokens),
        "max_text_tokens_per_sentence": int(max_text_tokens_per_sentence),
        "sentences_bucket_max_size": int(sentences_bucket_max_size)
    }
    
    # æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—
    task_id = task_queue.add_task(prompt, text, infer_mode, params)
    
    gr.Info(f"ğŸ¯ ä»»åŠ¡ {task_id} å·²æäº¤åˆ°é˜Ÿåˆ—")
    
    # è½®è¯¢ç­‰å¾…ä»»åŠ¡å®Œæˆ
    max_wait_time = 300  # æœ€å¤§ç­‰å¾…5åˆ†é’Ÿ
    start_wait = time.time()
    
    while time.time() - start_wait < max_wait_time:
        task = task_queue.get_task_result(task_id)
        
        if task and task.status == TaskStatus.COMPLETED:
            gr.Info(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆï¼")
            return gr.update(value=task.result_path, visible=True), f"ä»»åŠ¡ {task_id} å·²å®Œæˆ"
        elif task and task.status == TaskStatus.FAILED:
            gr.Error(f"âŒ ä»»åŠ¡ {task_id} å¤±è´¥: {task.error_message}")
            return gr.update(value=None, visible=True), f"ä»»åŠ¡å¤±è´¥: {task.error_message}"
        
        # æ›´æ–°è¿›åº¦ä¿¡æ¯
        if task and task.status == TaskStatus.RUNNING:
            elapsed = time.time() - task.start_time
            progress(0.5, f"ä»»åŠ¡ {task_id} æ‰§è¡Œä¸­... ({elapsed:.1f}s)")
        else:
            queue_status = task_queue.get_queue_status()
            if queue_status['queue_size'] > 0:
                progress(0.1, f"æ’é˜Ÿä¸­... å‰é¢è¿˜æœ‰ {queue_status['queue_size']} ä¸ªä»»åŠ¡")
        
        time.sleep(1)
    
    gr.Error(f"âŒ ä»»åŠ¡ {task_id} è¶…æ—¶")
    return gr.update(value=None, visible=True), "ä»»åŠ¡è¶…æ—¶"

def get_queue_status_display():
    """è·å–é˜Ÿåˆ—çŠ¶æ€æ˜¾ç¤ºä¿¡æ¯"""
    status = task_queue.get_queue_status()
    
    if status['current_task']:
        current_info = status['current_task']
        current_text = f"""
        ğŸ”„ **å½“å‰æ‰§è¡Œä»»åŠ¡**
        - ä»»åŠ¡ID: {current_info['id']}
        - å†…å®¹é¢„è§ˆ: {current_info['text_preview']}
        - å·²æ‰§è¡Œæ—¶é—´: {current_info['elapsed_time']:.1f}s
        - é¢„ä¼°å‰©ä½™: {current_info['estimated_remaining']:.1f}s
        """
    else:
        current_text = "ğŸ’¤ **å½“å‰æ— ä»»åŠ¡æ‰§è¡Œ**"
    
    queue_info = f"""
    ğŸ“Š **é˜Ÿåˆ—çŠ¶æ€**
    - æ’é˜Ÿä»»åŠ¡æ•°: {status['queue_size']}
    - å·²å®Œæˆä»»åŠ¡: {status['total_completed']}
    - å¹³å‡æ‰§è¡Œæ—¶é—´: {status['average_execution_time']:.1f}s
    - é¢„ä¼°ç­‰å¾…æ—¶é—´: {status['estimated_wait_time']:.1f}s
    """
    
    return current_text + "\n" + queue_info

def refresh_queue_status():
    """åˆ·æ–°é˜Ÿåˆ—çŠ¶æ€"""
    return get_queue_status_display()

def update_prompt_audio():
    """æ›´æ–°æç¤ºéŸ³é¢‘æŒ‰é’®çŠ¶æ€"""
    return gr.update(interactive=True)

def on_input_text_change(text, max_tokens_per_sentence):
    """æ–‡æœ¬è¾“å…¥å˜åŒ–æ—¶çš„å›è°ƒå‡½æ•°"""
    if text and len(text.strip()) > 0:
        try:
            text_tokens_list = tts.tokenizer.tokenize(text)
            sentences = tts.tokenizer.split_sentences(
                text_tokens_list, 
                max_tokens_per_sentence=int(max_tokens_per_sentence)
            )
            
            data = []
            for i, s in enumerate(sentences):
                sentence_str = ''.join(s)
                tokens_count = len(s)
                data.append([i + 1, sentence_str, tokens_count])
            
            return gr.update(value=data, visible=True)
        except Exception as e:
            print(f"âš ï¸  æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
            return gr.update(value=[], visible=True)
    else:
        return gr.update(value=[], visible=True)

def on_demo_category_change(category):
    """å½“demosåˆ†ç±»æ”¹å˜æ—¶æ›´æ–°å­åˆ†ç±»é€‰é¡¹"""
    if category:
        subcategories = get_demo_subcategories(category)
        return gr.update(choices=subcategories, value=subcategories[0] if subcategories else None)
    return gr.update(choices=[], value=None)

def on_demo_subcategory_change(category, subcategory):
    """å½“demoså­åˆ†ç±»æ”¹å˜æ—¶æ›´æ–°éŸ³é¢‘æ–‡ä»¶é€‰é¡¹"""
    if category and subcategory:
        audio_files = get_demo_audio_files(category, subcategory)
        return gr.update(choices=audio_files, value=audio_files[0] if audio_files else None)
    return gr.update(choices=[], value=None)

def on_demo_audio_select(category, subcategory, filename):
    """å½“é€‰æ‹©demoséŸ³é¢‘æ—¶æ›´æ–°éŸ³é¢‘ç»„ä»¶"""
    if category and subcategory and filename:
        audio_path = get_demo_audio_path(category, subcategory, filename)
        if audio_path and os.path.exists(audio_path):
            return gr.update(value=audio_path)
    return gr.update(value=None)

def clear_text():
    """æ¸…ç©ºç›®æ ‡æ–‡æœ¬æ¡†å†…å®¹"""
    return gr.update(value="")

def auto_use_demo_audio(category, subcategory, filename):
    """å½“éŸ³é¢‘æ–‡ä»¶è¢«é€‰ä¸­æ—¶è‡ªåŠ¨ä½¿ç”¨è¯¥éŸ³é¢‘"""
    if category and subcategory and filename:
        audio_path = get_demo_audio_path(category, subcategory, filename)
        if audio_path and os.path.exists(audio_path):
            return gr.update(value=audio_path)
    return gr.update(value=None)

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(
    title="IndexTTS Demo - ç»Ÿä¸€ç‰ˆæœ¬", 
    theme=gr.themes.Soft(),
    css="""
    .gradio-container {
        max-width: 1200px !important;
    }
    .header {
        text-align: center;
        margin-bottom: 20px;
    }
    .demos-section {
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 15px;
        margin-bottom: 15px;
        background-color: #f9f9f9;
    }
    .queue-status {
        border: 1px solid #2196f3;
        border-radius: 8px;
        padding: 15px;
        margin-bottom: 15px;
        background-color: #e3f2fd;
    }
    """
) as demo:
    mutex = threading.Lock()
    
    gr.HTML('''
    <div class="header">
        <h1>ğŸ¤ IndexTTS: å·¥ä¸šçº§é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿ</h1>
        <h3>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot TTS System</h3>
        <p>
            <a href='https://arxiv.org/abs/2502.05512'><img src='https://img.shields.io/badge/ArXiv-2502.05512-red'></a>
            <img src='https://img.shields.io/badge/Status-Fixed-green'>
            <img src='https://img.shields.io/badge/Docker-Supported-blue'>
            <img src='https://img.shields.io/badge/Demos-Enabled-orange'>
            <img src='https://img.shields.io/badge/Queue-Enabled-purple'>
        </p>
        <p><strong>âœ… å·²ä¿®å¤ bitsandbytes å…¼å®¹æ€§é—®é¢˜ | æ”¯æŒ Docker éƒ¨ç½² | ğŸµ æ”¯æŒé¢„è®¾éŸ³é¢‘é€‰æ‹© | ğŸ“‹ æ™ºèƒ½é˜Ÿåˆ—ç®¡ç†</strong></p>
    </div>
    ''')
    
    with gr.Tab("ğŸµ éŸ³é¢‘ç”Ÿæˆ"):
        # é˜Ÿåˆ—çŠ¶æ€æ˜¾ç¤º
        with gr.Group():
            gr.Markdown("### ğŸ“‹ é˜Ÿåˆ—çŠ¶æ€", elem_classes=["queue-status"])
            queue_status_display = gr.Markdown(
                get_queue_status_display(),
                elem_classes=["queue-status"]
            )
            with gr.Row():
                refresh_queue_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", size="sm")
                task_status_output = gr.Textbox(
                    label="ä»»åŠ¡çŠ¶æ€", 
                    interactive=False,
                    placeholder="ä»»åŠ¡çŠ¶æ€å°†åœ¨è¿™é‡Œæ˜¾ç¤º..."
                )
        
        with gr.Row():
            with gr.Column(scale=1):
                # é¢„è®¾éŸ³é¢‘é€‰æ‹©åŒºåŸŸ
                if get_demo_categories():
                    with gr.Group():
                        gr.Markdown("### ğŸ­ é€‰æ‹©é¢„è®¾éŸ³é¢‘")
                        
                        # è·å–åˆå§‹åŒ–å€¼
                        initial_categories = get_demo_categories()
                        initial_category = initial_categories[0] if initial_categories else None
                        initial_subcategories = get_demo_subcategories(initial_category) if initial_category else []
                        initial_subcategory = initial_subcategories[0] if initial_subcategories else None
                        initial_audio_files = get_demo_audio_files(initial_category, initial_subcategory) if initial_category and initial_subcategory else []
                        initial_audio_file = initial_audio_files[0] if initial_audio_files else None
                        
                        with gr.Row():
                            demo_category = gr.Dropdown(
                                choices=initial_categories,
                                label="åˆ†ç±»",
                                value=initial_category
                            )
                            demo_subcategory = gr.Dropdown(
                                choices=initial_subcategories,
                                label="å­åˆ†ç±»",
                                value=initial_subcategory
                            )
                        demo_audio_file = gr.Dropdown(
                            choices=initial_audio_files,
                            label="éŸ³é¢‘æ–‡ä»¶",
                            value=initial_audio_file
                        )
                        use_demo_btn = gr.Button("ğŸ¯ ä½¿ç”¨é€‰ä¸­éŸ³é¢‘", variant="secondary", size="sm")
                
                gr.Markdown("### ğŸ“ ä¸Šä¼ /å½•åˆ¶éŸ³é¢‘")
                prompt_audio = gr.Audio(
                    label="å‚è€ƒéŸ³é¢‘", 
                    sources=["upload", "microphone"],
                    type="filepath"
                )
                
            with gr.Column(scale=2):
                with gr.Row():
                    input_text_single = gr.TextArea(
                        label="ğŸ“ ç›®æ ‡æ–‡æœ¬",
                        placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                        info=f"å½“å‰æ¨¡å‹ç‰ˆæœ¬: {getattr(tts, 'model_version', None) or '1.0'}",
                        lines=6,
                        scale=4
                    )
                
                with gr.Row():
                    infer_mode = gr.Radio(
                        choices=["æ™®é€šæ¨ç†", "æ‰¹æ¬¡æ¨ç†"], 
                        label="âš¡ æ¨ç†æ¨¡å¼",
                        info="æ‰¹æ¬¡æ¨ç†ï¼šæ›´é€‚åˆé•¿å¥ï¼Œæ€§èƒ½ç¿»å€",
                        value="æ™®é€šæ¨ç†"
                    )

                with gr.Row():
                    gen_button = gr.Button("ğŸ¯ æäº¤åˆ°é˜Ÿåˆ—", variant="primary", size="lg")
                    clear_text_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary", size="lg")
        
        output_audio = gr.Audio(label="ğŸµ ç”Ÿæˆç»“æœ", visible=True)
        
        # é«˜çº§å‚æ•°è®¾ç½®
        with gr.Accordion("âš™ï¸ é«˜çº§å‚æ•°è®¾ç½®", open=False):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("**ğŸ›ï¸ GPT2 é‡‡æ ·è®¾ç½®**")
                    with gr.Row():
                        do_sample = gr.Checkbox(label="å¯ç”¨é‡‡æ ·", value=True)
                        temperature = gr.Slider(
                            label="æ¸©åº¦", minimum=0.1, maximum=2.0, value=1.0, step=0.1
                        )
                    with gr.Row():
                        top_p = gr.Slider(
                            label="Top-p", minimum=0.0, maximum=1.0, value=0.8, step=0.01
                        )
                        top_k = gr.Slider(
                            label="Top-k", minimum=0, maximum=100, value=30, step=1
                        )
                    with gr.Row():
                        num_beams = gr.Slider(
                            label="æŸæœç´¢æ•°é‡", value=3, minimum=1, maximum=10, step=1
                        )
                        repetition_penalty = gr.Number(
                            label="é‡å¤æƒ©ç½š", value=10.0, minimum=0.1, maximum=20.0, step=0.1
                        )
                    with gr.Row():
                        length_penalty = gr.Number(
                            label="é•¿åº¦æƒ©ç½š", value=0.0, minimum=-2.0, maximum=2.0, step=0.1
                        )
                        max_mel_tokens = gr.Slider(
                            label="æœ€å¤§Mel tokenæ•°", 
                            value=600, 
                            minimum=50, 
                            maximum=getattr(tts.cfg.gpt, 'max_mel_tokens', 1000), 
                            step=10
                        )
                
                with gr.Column(scale=1):
                    gr.Markdown("**âœ‚ï¸ åˆ†å¥è®¾ç½®**")
                    max_text_tokens_per_sentence = gr.Slider(
                        label="åˆ†å¥æœ€å¤§Tokenæ•°", 
                        value=120, 
                        minimum=20, 
                        maximum=getattr(tts.cfg.gpt, 'max_text_tokens', 300), 
                        step=2,
                        info="å»ºè®®80~200ä¹‹é—´"
                    )
                    sentences_bucket_max_size = gr.Slider(
                        label="åˆ†å¥åˆ†æ¡¶å¤§å°ï¼ˆæ‰¹æ¬¡æ¨ç†ï¼‰", 
                        value=4, 
                        minimum=1, 
                        maximum=16, 
                        step=1,
                        info="å»ºè®®2-8ä¹‹é—´"
                    )
                    
                    with gr.Accordion("ğŸ“‹ åˆ†å¥é¢„è§ˆ", open=True):
                        sentences_preview = gr.Dataframe(
                            headers=["åºå·", "åˆ†å¥å†…å®¹", "Tokenæ•°"],
                            wrap=True,
                            interactive=False
                        )
        
        # ç¤ºä¾‹æ¡ˆä¾‹
        if len(example_cases) > 0:
            gr.Examples(
                examples=example_cases,
                inputs=[input_text_single, infer_mode],  # ç§»é™¤ prompt_audio
                label="ğŸ“š ç¤ºä¾‹æ¡ˆä¾‹"
            )
    
    with gr.Tab("ğŸ­ éŸ³é¢‘åº“ç®¡ç†"):
        gr.Markdown("### ğŸ“ Demos éŸ³é¢‘åº“")
        
        # æ˜¾ç¤ºå½“å‰éŸ³é¢‘åº“ç»Ÿè®¡
        total_categories, total_files, demos_audio_dict = get_demos_statistics()
        
        gr.Markdown(f"""
        **ğŸ“Š ç»Ÿè®¡ä¿¡æ¯**
        - åˆ†ç±»æ•°é‡: {total_categories}
        - éŸ³é¢‘æ–‡ä»¶æ€»æ•°: {total_files}
        """)
        
        # æ˜¾ç¤ºéŸ³é¢‘åº“ç»“æ„
        if demos_audio_dict:
            structure_md = "**ğŸ“‚ ç›®å½•ç»“æ„**\n\n"
            for category, subcategories in demos_audio_dict.items():
                structure_md += f"- **{category}**\n"
                for subcategory, files in subcategories.items():
                    structure_md += f"  - {subcategory} ({len(files)} ä¸ªæ–‡ä»¶)\n"
                    for file_info in files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                        structure_md += f"    - {file_info['name']}\n"
                    if len(files) > 3:
                        structure_md += f"    - ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶\n"
            
            gr.Markdown(structure_md)
        else:
            gr.Markdown("**âš ï¸ æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶**\n\nè¯·åœ¨ `demos/` ç›®å½•ä¸‹æ·»åŠ éŸ³é¢‘æ–‡ä»¶ã€‚")
        
        gr.Markdown("""
        ### ğŸ“ æ·»åŠ éŸ³é¢‘æ–‡ä»¶
        
        1. **å‡†å¤‡éŸ³é¢‘æ–‡ä»¶**
           - æ ¼å¼ï¼šWAV
           - é‡‡æ ·ç‡ï¼š22050Hz æˆ– 44100Hz
           - æ—¶é•¿ï¼š3-10ç§’
           - è´¨é‡ï¼šæ¸…æ™°æ— å™ªéŸ³
        
        2. **æ–‡ä»¶å‘½å**
           - ä¸­æ–‡ï¼š`è¯´è¯äºº-å†…å®¹æè¿°.wav`
           - è‹±æ–‡ï¼š`Speaker-Content.wav`
           - è§’è‰²ï¼š`è§’è‰²å-å°è¯å†…å®¹.wav`
        
        3. **æ”¾ç½®æ–‡ä»¶**
           - å°†æ–‡ä»¶æ”¾å…¥å¯¹åº”çš„åˆ†ç±»ç›®å½•
           - é‡å¯ WebUI ä»¥åˆ·æ–°åˆ—è¡¨
        """)
    
    with gr.Tab("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯"):
        with gr.Row():
            with gr.Column():
                gr.Markdown(f"""
                ### ğŸ–¥ï¸ ç³»ç»Ÿä¿¡æ¯
                - **Python ç‰ˆæœ¬**: {sys.version}
                - **å·¥ä½œç›®å½•**: {os.getcwd()}
                - **æ¨¡å‹ç›®å½•**: {cmd_args.model_dir}
                - **GPU å¯ç”¨**: {torch.cuda.is_available()}
                - **GPU æ•°é‡**: {torch.cuda.device_count() if torch.cuda.is_available() else 0}
                - **CUDA ç‰ˆæœ¬**: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}
                
                ### ğŸ“Š æ¨¡å‹ä¿¡æ¯
                - **æ¨¡å‹ç‰ˆæœ¬**: {getattr(tts, 'model_version', None) or '1.0'}
                - **è®¾å¤‡**: {getattr(tts, 'device', 'unknown')}
                - **FP16**: {getattr(tts, 'is_fp16', False)}
                - **CUDA å†…æ ¸**: {getattr(tts, 'use_cuda_kernel', False)}
                
                ### ğŸ”§ ä¿®å¤çŠ¶æ€
                - **bitsandbytes**: âœ… å·²ç¦ç”¨
                - **DeepSpeed**: âš ï¸ æœªå®‰è£… (è‡ªåŠ¨å›é€€)
                - **BigVGAN CUDA**: âš ï¸ å›é€€åˆ° torch å®ç°
                
                ### ğŸ“‹ é˜Ÿåˆ—ç®¡ç†
                - **é˜Ÿåˆ—åŠŸèƒ½**: âœ… å·²å¯ç”¨
                - **å¹¶å‘é™åˆ¶**: 1ä¸ªä»»åŠ¡
                - **å†å²ç»Ÿè®¡**: æœ€è¿‘20æ¬¡æ‰§è¡Œæ—¶é—´
                - **æ—¶é—´é¢„ä¼°**: åŸºäºå†å²ç®—åŠ›
                
                ### ğŸ­ Demos åŠŸèƒ½
                - **éŸ³é¢‘åˆ†ç±»**: {total_categories}
                - **æ€»éŸ³é¢‘æ•°**: {total_files}
                - **æ”¯æŒæ ¼å¼**: WAV
                
                ### ğŸ“‹ å¯åŠ¨å‚æ•°
                - **ä¸»æœº**: {cmd_args.host}
                - **ç«¯å£**: {cmd_args.port}
                - **è¯¦ç»†æ¨¡å¼**: {cmd_args.verbose}
                - **åˆ†äº«é“¾æ¥**: {cmd_args.share}
                """)
    
    # é«˜çº§å‚æ•°åˆ—è¡¨
    advanced_params = [
        do_sample, top_p, top_k, temperature,
        length_penalty, num_beams, repetition_penalty, max_mel_tokens,
    ]
    
    # äº‹ä»¶ç»‘å®š
    input_text_single.change(
        on_input_text_change,
        inputs=[input_text_single, max_text_tokens_per_sentence],
        outputs=[sentences_preview]
    )
    
    max_text_tokens_per_sentence.change(
        on_input_text_change,
        inputs=[input_text_single, max_text_tokens_per_sentence],
        outputs=[sentences_preview]
    )
    
    prompt_audio.upload(
        update_prompt_audio,
        inputs=[],
        outputs=[gen_button]
    )
    
    # æ¸…ç©ºæ–‡æœ¬æŒ‰é’®äº‹ä»¶ç»‘å®š
    clear_text_btn.click(
        clear_text,
        inputs=[],
        outputs=[input_text_single]
    )
    
    # é˜Ÿåˆ—çŠ¶æ€åˆ·æ–°
    refresh_queue_btn.click(
        refresh_queue_status,
        inputs=[],
        outputs=[queue_status_display]
    )
    
    # Demos ç›¸å…³äº‹ä»¶ç»‘å®šï¼ˆåªæœ‰åœ¨æœ‰demoséŸ³é¢‘æ—¶æ‰ç»‘å®šï¼‰
    if get_demo_categories():
        demo_category.change(
            on_demo_category_change,
            inputs=[demo_category],
            outputs=[demo_subcategory]
        )
        
        demo_subcategory.change(
            on_demo_subcategory_change,
            inputs=[demo_category, demo_subcategory],
            outputs=[demo_audio_file]
        )
        
        # éŸ³é¢‘æ–‡ä»¶é€‰æ‹©æ—¶è‡ªåŠ¨ä½¿ç”¨è¯¥éŸ³é¢‘
        demo_audio_file.change(
            auto_use_demo_audio,
            inputs=[demo_category, demo_subcategory, demo_audio_file],
            outputs=[prompt_audio]
        )
        
        # ä¿ç•™æ‰‹åŠ¨ä½¿ç”¨æŒ‰é’®åŠŸèƒ½
        use_demo_btn.click(
            on_demo_audio_select,
            inputs=[demo_category, demo_subcategory, demo_audio_file],
            outputs=[prompt_audio]
        )
    
    # ä¿®æ”¹ç”ŸæˆæŒ‰é’®äº‹ä»¶ï¼Œä½¿ç”¨é˜Ÿåˆ—ç‰ˆæœ¬
    gen_button.click(
        gen_single_with_queue,
        inputs=[
            prompt_audio, input_text_single, infer_mode,
            max_text_tokens_per_sentence, sentences_bucket_max_size,
            *advanced_params,
        ],
        outputs=[output_audio, task_status_output]
    )
    
    # åˆå§‹åŒ–é˜Ÿåˆ—çŠ¶æ€æ˜¾ç¤º
    demo.load(
        refresh_queue_status,
        inputs=[],
        outputs=[queue_status_display]
    )

if __name__ == "__main__":
    print("\nğŸŒ å¯åŠ¨ IndexTTS WebUI (ç»Ÿä¸€ç‰ˆæœ¬)...")
    print(f"   åœ°å€: http://{cmd_args.host}:{cmd_args.port}")
    print(f"   æ¨¡å‹ç›®å½•: {cmd_args.model_dir}")
    print(f"   è¯¦ç»†æ¨¡å¼: {cmd_args.verbose}")
    print(f"   åˆ†äº«é“¾æ¥: {cmd_args.share}")
    print(f"   é˜Ÿåˆ—ç®¡ç†: âœ… å·²å¯ç”¨")
    
    # è·å–demosç»Ÿè®¡ä¿¡æ¯
    total_categories, total_files, _ = get_demos_statistics()
    print(f"   éŸ³é¢‘åˆ†ç±»: {total_categories}")
    print(f"   éŸ³é¢‘æ–‡ä»¶: {total_files}")
    print("   æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 50)
    
    try:
        demo.queue(max_size=20)
        demo.launch(
            server_name=cmd_args.host,
            server_port=cmd_args.port,
            share=cmd_args.share,
            inbrowser=not cmd_args.share,  # å¦‚æœä¸åˆ†äº«åˆ™è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
            show_error=True
        )
    finally:
        # ç¡®ä¿é˜Ÿåˆ—å¤„ç†å™¨æ­£å¸¸åœæ­¢
        task_queue.stop_worker()
        print("ğŸ›‘ é˜Ÿåˆ—å¤„ç†å™¨å·²åœæ­¢")
