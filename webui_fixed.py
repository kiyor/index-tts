#!/usr/bin/env python3
"""
IndexTTS WebUI - ä¿®å¤ç‰ˆæœ¬
åŒ…å« bitsandbytes å…¼å®¹æ€§ä¿®å¤å’Œ Docker æ”¯æŒ
"""

# ä¿®å¤ bitsandbytes å…¼å®¹æ€§é—®é¢˜
import sys
sys.modules['bitsandbytes'] = None

import json
import os
import sys
import threading
import time

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, "indextts"))

import argparse
parser = argparse.ArgumentParser(description="IndexTTS WebUI (Fixed)")
parser.add_argument("--verbose", action="store_true", default=False, help="Enable verbose mode")
parser.add_argument("--port", type=int, default=7860, help="Port to run the web UI on")
parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to run the web UI on (0.0.0.0 for Docker)")
parser.add_argument("--model_dir", type=str, default="checkpoints", help="Model checkpoints directory")
parser.add_argument("--share", action="store_true", default=False, help="Create a publicly shareable link")
cmd_args = parser.parse_args()

print("ğŸš€ IndexTTS WebUI (ä¿®å¤ç‰ˆæœ¬)")
print("=" * 50)

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
if not os.path.exists(cmd_args.model_dir):
    print(f"âŒ æ¨¡å‹ç›®å½• {cmd_args.model_dir} ä¸å­˜åœ¨ã€‚è¯·å…ˆä¸‹è½½æ¨¡å‹ã€‚")
    sys.exit(1)

required_files = [
    "bigvgan_generator.pth",
    "bpe.model", 
    "gpt.pth",
    "config.yaml",
]

missing_files = []
for file in required_files:
    file_path = os.path.join(cmd_args.model_dir, file)
    if not os.path.exists(file_path):
        missing_files.append(file_path)

if missing_files:
    print("âŒ ç¼ºå°‘å¿…éœ€æ–‡ä»¶:")
    for file in missing_files:
        print(f"   - {file}")
    print("\nè¯·ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ–‡ä»¶éƒ½å·²ä¸‹è½½åˆ° checkpoints ç›®å½•")
    sys.exit(1)

print("âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")

# æ£€æŸ¥ GPU
try:
    import torch
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ª GPU: {gpu_name}")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼")
except Exception as e:
    print(f"âš ï¸  GPU æ£€æŸ¥å¤±è´¥: {e}")

import gradio as gr

try:
    from indextts.infer import IndexTTS
    print("âœ… IndexTTS å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ IndexTTS å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# å°è¯•å¯¼å…¥ i18nï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ
try:
    from tools.i18n.i18n import I18nAuto
    i18n = I18nAuto(language="zh_CN")
    print("âœ… i18n æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError:
    print("âš ï¸  i18n æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€")
    class SimpleI18n:
        def __call__(self, text):
            return text
    i18n = SimpleI18n()

print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– IndexTTS...")
try:
    tts = IndexTTS(
        model_dir=cmd_args.model_dir, 
        cfg_path=os.path.join(cmd_args.model_dir, "config.yaml")
    )
    print("âœ… IndexTTS åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âŒ IndexTTS åˆå§‹åŒ–å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("outputs", exist_ok=True)
os.makedirs("prompts", exist_ok=True)
os.makedirs("logs", exist_ok=True)

# åŠ è½½ç¤ºä¾‹æ¡ˆä¾‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
example_cases = []
if os.path.exists("tests/cases.jsonl"):
    try:
        with open("tests/cases.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                example = json.loads(line)
                example_cases.append([
                    os.path.join("tests", example.get("prompt_audio", "sample_prompt.wav")),
                    example.get("text"), 
                    ["æ™®é€šæ¨ç†", "æ‰¹æ¬¡æ¨ç†"][example.get("infer_mode", 0)]
                ])
        print(f"âœ… åŠ è½½äº† {len(example_cases)} ä¸ªç¤ºä¾‹æ¡ˆä¾‹")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½ç¤ºä¾‹æ¡ˆä¾‹å¤±è´¥: {e}")

def gen_single(prompt, text, infer_mode, max_text_tokens_per_sentence=120, sentences_bucket_max_size=4,
                *args, progress=gr.Progress()):
    """ç”Ÿæˆè¯­éŸ³çš„ä¸»å‡½æ•°"""
    if not prompt:
        gr.Warning("è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘æ–‡ä»¶")
        return gr.update(value=None, visible=True)
    
    if not text or not text.strip():
        gr.Warning("è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬")
        return gr.update(value=None, visible=True)
    
    output_path = os.path.join("outputs", f"spk_{int(time.time())}.wav")
    
    # è®¾ç½®è¿›åº¦æ¡
    tts.gr_progress = progress
    
    # è§£æå‚æ•°
    do_sample, top_p, top_k, temperature, \
        length_penalty, num_beams, repetition_penalty, max_mel_tokens = args
    
    kwargs = {
        "do_sample": bool(do_sample),
        "top_p": float(top_p),
        "top_k": int(top_k) if int(top_k) > 0 else None,
        "temperature": float(temperature),
        "length_penalty": float(length_penalty),
        "num_beams": int(num_beams),
        "repetition_penalty": float(repetition_penalty),
        "max_mel_tokens": int(max_mel_tokens),
    }
    
    try:
        start_time = time.time()
        if infer_mode == "æ™®é€šæ¨ç†":
            output = tts.infer(
                prompt, text, output_path, 
                verbose=cmd_args.verbose,
                max_text_tokens_per_sentence=int(max_text_tokens_per_sentence),
                **kwargs
            )
        else:
            # æ‰¹æ¬¡æ¨ç†
            output = tts.infer_fast(
                prompt, text, output_path, 
                verbose=cmd_args.verbose,
                max_text_tokens_per_sentence=int(max_text_tokens_per_sentence),
                sentences_bucket_max_size=int(sentences_bucket_max_size),
                **kwargs
            )
        
        end_time = time.time()
        gr.Info(f"âœ… ç”Ÿæˆå®Œæˆï¼è€—æ—¶ {end_time - start_time:.2f} ç§’")
        return gr.update(value=output, visible=True)
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        gr.Error(error_msg)
        return gr.update(value=None, visible=True)

def update_prompt_audio():
    """æ›´æ–°æç¤ºéŸ³é¢‘æŒ‰é’®çŠ¶æ€"""
    return gr.update(interactive=True)

def on_input_text_change(text, max_tokens_per_sentence):
    """æ–‡æœ¬è¾“å…¥å˜åŒ–æ—¶çš„å›è°ƒå‡½æ•°"""
    if text and len(text.strip()) > 0:
        try:
            text_tokens_list = tts.tokenizer.tokenize(text)
            sentences = tts.tokenizer.split_sentences(
                text_tokens_list, 
                max_tokens_per_sentence=int(max_tokens_per_sentence)
            )
            
            data = []
            for i, s in enumerate(sentences):
                sentence_str = ''.join(s)
                tokens_count = len(s)
                data.append([i + 1, sentence_str, tokens_count])
            
            return gr.update(value=data, visible=True)
        except Exception as e:
            print(f"âš ï¸  æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
            return gr.update(value=[], visible=True)
    else:
        return gr.update(value=[], visible=True)

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(
    title="IndexTTS Demo - Fixed", 
    theme=gr.themes.Soft(),
    css="""
    .gradio-container {
        max-width: 1200px !important;
    }
    .header {
        text-align: center;
        margin-bottom: 20px;
    }
    """
) as demo:
    gr.HTML('''
    <div class="header">
        <h1>ğŸ¤ IndexTTS: å·¥ä¸šçº§é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿ</h1>
        <h3>IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot TTS System</h3>
        <p>
            <a href='https://arxiv.org/abs/2502.05512'><img src='https://img.shields.io/badge/ArXiv-2502.05512-red'></a>
            <img src='https://img.shields.io/badge/Status-Fixed-green'>
            <img src='https://img.shields.io/badge/Docker-Supported-blue'>
        </p>
        <p><strong>âœ… å·²ä¿®å¤ bitsandbytes å…¼å®¹æ€§é—®é¢˜ | æ”¯æŒ Docker éƒ¨ç½²</strong></p>
    </div>
    ''')
    
    with gr.Tab("ğŸµ éŸ³é¢‘ç”Ÿæˆ"):
        with gr.Row():
            with gr.Column(scale=1):
                prompt_audio = gr.Audio(
                    label="ğŸ“ å‚è€ƒéŸ³é¢‘", 
                    sources=["upload", "microphone"],
                    type="filepath"
                )
                
            with gr.Column(scale=2):
                input_text_single = gr.TextArea(
                    label="ğŸ“ ç›®æ ‡æ–‡æœ¬",
                    placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                    info=f"å½“å‰æ¨¡å‹ç‰ˆæœ¬: {getattr(tts, 'model_version', None) or '1.0'}",
                    lines=3
                )
                
                with gr.Row():
                    infer_mode = gr.Radio(
                        choices=["æ™®é€šæ¨ç†", "æ‰¹æ¬¡æ¨ç†"], 
                        label="âš¡ æ¨ç†æ¨¡å¼",
                        info="æ‰¹æ¬¡æ¨ç†ï¼šæ›´é€‚åˆé•¿å¥ï¼Œæ€§èƒ½ç¿»å€",
                        value="æ™®é€šæ¨ç†"
                    )
                    gen_button = gr.Button("ğŸ¯ ç”Ÿæˆè¯­éŸ³", variant="primary", size="lg")
        
        output_audio = gr.Audio(label="ğŸµ ç”Ÿæˆç»“æœ", visible=True)
        
        # é«˜çº§å‚æ•°è®¾ç½®
        with gr.Accordion("âš™ï¸ é«˜çº§å‚æ•°è®¾ç½®", open=False):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("**ğŸ›ï¸ GPT2 é‡‡æ ·è®¾ç½®**")
                    with gr.Row():
                        do_sample = gr.Checkbox(label="å¯ç”¨é‡‡æ ·", value=True)
                        temperature = gr.Slider(
                            label="æ¸©åº¦", minimum=0.1, maximum=2.0, value=1.0, step=0.1
                        )
                    with gr.Row():
                        top_p = gr.Slider(
                            label="Top-p", minimum=0.0, maximum=1.0, value=0.8, step=0.01
                        )
                        top_k = gr.Slider(
                            label="Top-k", minimum=0, maximum=100, value=30, step=1
                        )
                    with gr.Row():
                        num_beams = gr.Slider(
                            label="æŸæœç´¢æ•°é‡", value=3, minimum=1, maximum=10, step=1
                        )
                        repetition_penalty = gr.Number(
                            label="é‡å¤æƒ©ç½š", value=10.0, minimum=0.1, maximum=20.0, step=0.1
                        )
                    with gr.Row():
                        length_penalty = gr.Number(
                            label="é•¿åº¦æƒ©ç½š", value=0.0, minimum=-2.0, maximum=2.0, step=0.1
                        )
                        max_mel_tokens = gr.Slider(
                            label="æœ€å¤§Mel tokenæ•°", 
                            value=600, 
                            minimum=50, 
                            maximum=getattr(tts.cfg.gpt, 'max_mel_tokens', 1000), 
                            step=10
                        )
                
                with gr.Column(scale=1):
                    gr.Markdown("**âœ‚ï¸ åˆ†å¥è®¾ç½®**")
                    max_text_tokens_per_sentence = gr.Slider(
                        label="åˆ†å¥æœ€å¤§Tokenæ•°", 
                        value=120, 
                        minimum=20, 
                        maximum=getattr(tts.cfg.gpt, 'max_text_tokens', 300), 
                        step=2,
                        info="å»ºè®®80~200ä¹‹é—´"
                    )
                    sentences_bucket_max_size = gr.Slider(
                        label="åˆ†å¥åˆ†æ¡¶å¤§å°ï¼ˆæ‰¹æ¬¡æ¨ç†ï¼‰", 
                        value=4, 
                        minimum=1, 
                        maximum=16, 
                        step=1,
                        info="å»ºè®®2-8ä¹‹é—´"
                    )
                    
                    with gr.Accordion("ğŸ“‹ åˆ†å¥é¢„è§ˆ", open=True):
                        sentences_preview = gr.Dataframe(
                            headers=["åºå·", "åˆ†å¥å†…å®¹", "Tokenæ•°"],
                            wrap=True,
                            interactive=False
                        )
        
        # ç¤ºä¾‹æ¡ˆä¾‹
        if len(example_cases) > 0:
            gr.Examples(
                examples=example_cases,
                inputs=[prompt_audio, input_text_single, infer_mode],
                label="ğŸ“š ç¤ºä¾‹æ¡ˆä¾‹"
            )
    
    with gr.Tab("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯"):
        with gr.Row():
            with gr.Column():
                gr.Markdown(f"""
                ### ğŸ–¥ï¸ ç³»ç»Ÿä¿¡æ¯
                - **Python ç‰ˆæœ¬**: {sys.version}
                - **å·¥ä½œç›®å½•**: {os.getcwd()}
                - **æ¨¡å‹ç›®å½•**: {cmd_args.model_dir}
                - **GPU å¯ç”¨**: {torch.cuda.is_available()}
                - **GPU æ•°é‡**: {torch.cuda.device_count() if torch.cuda.is_available() else 0}
                - **CUDA ç‰ˆæœ¬**: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}
                
                ### ğŸ“Š æ¨¡å‹ä¿¡æ¯
                - **æ¨¡å‹ç‰ˆæœ¬**: {getattr(tts, 'model_version', None) or '1.0'}
                - **è®¾å¤‡**: {getattr(tts, 'device', 'unknown')}
                - **FP16**: {getattr(tts, 'is_fp16', False)}
                - **CUDA å†…æ ¸**: {getattr(tts, 'use_cuda_kernel', False)}
                
                ### ğŸ”§ ä¿®å¤çŠ¶æ€
                - **bitsandbytes**: âœ… å·²ç¦ç”¨
                - **DeepSpeed**: âš ï¸ æœªå®‰è£… (è‡ªåŠ¨å›é€€)
                - **BigVGAN CUDA**: âš ï¸ å›é€€åˆ° torch å®ç°
                """)
    
    # é«˜çº§å‚æ•°åˆ—è¡¨
    advanced_params = [
        do_sample, top_p, top_k, temperature,
        length_penalty, num_beams, repetition_penalty, max_mel_tokens,
    ]
    
    # äº‹ä»¶ç»‘å®š
    input_text_single.change(
        on_input_text_change,
        inputs=[input_text_single, max_text_tokens_per_sentence],
        outputs=[sentences_preview]
    )
    
    max_text_tokens_per_sentence.change(
        on_input_text_change,
        inputs=[input_text_single, max_text_tokens_per_sentence],
        outputs=[sentences_preview]
    )
    
    prompt_audio.upload(
        update_prompt_audio,
        inputs=[],
        outputs=[gen_button]
    )
    
    gen_button.click(
        gen_single,
        inputs=[
            prompt_audio, input_text_single, infer_mode,
            max_text_tokens_per_sentence, sentences_bucket_max_size,
            *advanced_params,
        ],
        outputs=[output_audio]
    )

if __name__ == "__main__":
    print("\nğŸŒ å¯åŠ¨ IndexTTS WebUI...")
    print(f"   åœ°å€: http://{cmd_args.host}:{cmd_args.port}")
    print(f"   æ¨¡å‹ç›®å½•: {cmd_args.model_dir}")
    print(f"   è¯¦ç»†æ¨¡å¼: {cmd_args.verbose}")
    print(f"   åˆ†äº«é“¾æ¥: {cmd_args.share}")
    print("   æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 50)
    
    demo.queue(max_size=20)
    demo.launch(
        server_name=cmd_args.host,
        server_port=cmd_args.port,
        share=cmd_args.share,
        inbrowser=not cmd_args.share,  # å¦‚æœä¸åˆ†äº«åˆ™è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
        show_error=True
    ) 